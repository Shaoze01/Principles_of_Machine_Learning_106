{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lab Assignment 2 - Part B: k-Nearest Neighbor Classification\n",
        "Please refer to the `README.pdf` for full laboratory instructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem Statement\n",
        "In this part, you will implement the k-Nearest Neighbor (k-NN) classifier and evaluate it on two datasets:\n",
        "- **Lenses Dataset**: A small dataset for contact lens prescription\n",
        "- **Credit Approval (CA) Dataset**: Credit card application data with binary labels (+/-)\n",
        "\n",
        "### Your Tasks\n",
        "1. **Preprocess the data**: Handle missing values and normalize features\n",
        "2. **Implement k-NN** with L2 distance\n",
        "3. **Evaluate** on both datasets for different values of k\n",
        "4. **Discuss** your results\n",
        "\n",
        "### Datasets\n",
        "The data files are located in the `credit 2017/` folder:\n",
        "- `lenses.training`, `lenses.testing`\n",
        "- `crx.data.training`, `crx.data.testing`\n",
        "- `crx.names` (describes the features)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Library declarations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lenses - Train: (18, 3), Test: (6, 3)\n",
            "Lenses - Train: (552, 15), Test: (138, 15)\n"
          ]
        }
      ],
      "source": [
        "# Data paths\n",
        "DATA_PATH = \"credit 2017/\"\n",
        "\n",
        "# Load Lenses data\n",
        "def load_lenses_data():\n",
        "    \"\"\"Load the lenses dataset.\"\"\"\n",
        "    train_data = np.loadtxt(DATA_PATH + \"lenses.training\", delimiter=',')\n",
        "    test_data = np.loadtxt(DATA_PATH + \"lenses.testing\", delimiter=',')\n",
        "    \n",
        "    # First column is ID, last column is label\n",
        "    X_train = train_data[:, 1:-1]\n",
        "    y_train = train_data[:, -1]\n",
        "    X_test = test_data[:, 1:-1]\n",
        "    y_test = test_data[:, -1]\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "# Load Credit Approval data\n",
        "def load_credit_data():\n",
        "    \"\"\"\n",
        "    Load the Credit Approval dataset.\n",
        "    Note: This dataset contains missing values (?) and mixed types.\n",
        "    You will need to preprocess it.\n",
        "    \"\"\"\n",
        "    # TODO: Implement data loading\n",
        "    # The data is comma-separated\n",
        "    # Missing values are marked with '?'\n",
        "    # Last column is the label ('+' or '-')\n",
        "    \n",
        "    train_data_crv = pd.read_csv(DATA_PATH+ \"crx.data.training\", header=None, na_values='?', dtype=str)\n",
        "    test_data_crv = pd.read_csv(DATA_PATH+ \"crx.data.testing\", header=None, na_values='?', dtype=str)\n",
        "    # test_data_crv = np.loadtxt(DATA_PATH , delimiter=',')\n",
        "        # First column is ID, last column is label\n",
        "    X_train = train_data_crv.iloc[:, :-1].to_numpy()\n",
        "    y_train = train_data_crv.iloc[:, -1].to_numpy()\n",
        "    X_test = test_data_crv.iloc[:, :-1].to_numpy()\n",
        "    y_test = test_data_crv.iloc[:, -1].to_numpy()\n",
        "    \n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Test loading lenses data\n",
        "X_train_lenses, y_train_lenses, X_test_lenses, y_test_lenses = load_lenses_data()\n",
        "print(f\"Lenses - Train: {X_train_lenses.shape}, Test: {X_test_lenses.shape}\")\n",
        "\n",
        "X_train_crv, y_train_crv, X_test_crv, y_test_crv = load_credit_data()\n",
        "print(f\"Lenses - Train: {X_train_crv.shape}, Test: {X_test_crv.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 1: Data Preprocessing\n",
        "For the Credit Approval dataset, you need to:\n",
        "1. **Handle missing values** (marked with '?'):\n",
        "   - Categorical features: replace with mode/median\n",
        "   - Numerical features: replace with label-conditioned mean\n",
        "2. **Normalize features** using z-scaling:\n",
        "   $$z_i^{(m)} = \\frac{x_i^{(m)} - \\mu_i}{\\sigma_i}$$\n",
        "\n",
        "Document exactly how you handle each feature!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_credit_data(train_file, test_file):\n",
        "    \"\"\"\n",
        "    Preprocess the Credit Approval dataset.\n",
        "    \n",
        "    Steps:\n",
        "    1. Load the data\n",
        "    2. Handle missing values\n",
        "    3. Encode categorical variables\n",
        "    4. Normalize numerical features\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    X_train, y_train, X_test, y_test : numpy arrays\n",
        "    \"\"\"\n",
        "    # TODO: Implement preprocessing\n",
        "    # Hint: Read crx.names to understand the features\n",
        "    # Feature types (from crx.names):\n",
        "    # A1: categorical (b, a)\n",
        "    # A2: continuous\n",
        "    # A3: continuous\n",
        "    # A4: categorical (u, y, l, t)\n",
        "    # A5: categorical (g, p, gg)\n",
        "    # A6: categorical (c, d, cc, i, j, k, m, r, q, w, x, e, aa, ff)\n",
        "    # A7: categorical (v, h, bb, j, n, z, dd, ff, o)\n",
        "    # A8: continuous\n",
        "    # A9: categorical (t, f)\n",
        "    # A10: categorical (t, f)\n",
        "    # A11: continuous\n",
        "    # A12: categorical (t, f)\n",
        "    # A13: categorical (g, p, s)\n",
        "    # A14: continuous\n",
        "    # A15: continuous\n",
        "    # A16: label (+, -)\n",
        "    \n",
        "    # Step 1 load data\n",
        "    train_df = pd.read_csv(train_file, header=None, na_values='?', dtype=str)\n",
        "    test_df = pd.read_csv(test_file, header=None, na_values='?', dtype=str)\n",
        "    num_cols = train_df.shape[1]\n",
        "    col_name = [f'A{i}' for i in range(1,num_cols+1)]\n",
        "    train_df.columns = col_name\n",
        "    test_df.columns = col_name\n",
        "    numerical_cols = ['A2', 'A3', 'A8', 'A11', 'A14', 'A15']\n",
        "    categorical_cols = ['A1', 'A4', 'A5', 'A6', 'A7', 'A9', 'A10', 'A12', 'A13']\n",
        "    label_col = 'A16'\n",
        "\n",
        "    # Step 2 missing data\n",
        "    # label\n",
        "    train_df[label_col] = train_df[label_col].map({'+': 1, '-': 0})\n",
        "    test_df[label_col] = test_df[label_col].map({'+': 1, '-': 0})\n",
        "    # \n",
        "    for col in numerical_cols:\n",
        "        train_df[col] = pd.to_numeric(train_df[col], errors='coerce')\n",
        "        test_df[col] = pd.to_numeric(test_df[col], errors='coerce')\n",
        "        # mean_val = train_df[col].mean()\n",
        "        # mean_val_test = test_df[col].mean()\n",
        "\n",
        "        # train_df[col] = train_df[col].fillna(mean_val)\n",
        "        # test_df[col] = test_df[col].fillna(mean_val_test)\n",
        "\n",
        "        train_df[col] = train_df[col].fillna(train_df.groupby(label_col)[col].transform('mean'))\n",
        "        test_df[col] = test_df[col].fillna(test_df.groupby(label_col)[col].transform('mean'))\n",
        "    # categorical\n",
        "    for col in categorical_cols:\n",
        "        mode_val = train_df[col].mode()[0]\n",
        "        mode_val_test = test_df[col].mode()[0]\n",
        "        train_df[col] = train_df[col].fillna(mode_val)\n",
        "        test_df[col] = test_df[col].fillna(mode_val_test)\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "    # Step 3: One-Hot Encoding\n",
        "    n_train = len(train_df)\n",
        "    n_test = len(test_df)\n",
        "\n",
        "    combined_df = pd.concat([train_df, test_df], axis=0)\n",
        "    combined_df_encoded = pd.get_dummies(combined_df, columns=categorical_cols)\n",
        "    \n",
        "    train_encoded = combined_df_encoded.iloc[:n_train, :].copy()\n",
        "    test_encoded = combined_df_encoded.iloc[n_test:, :].copy()\n",
        "\n",
        "\n",
        "\n",
        "    # Step 4: Z-Score normalization\n",
        "    for col in numerical_cols:\n",
        "        mean = train_encoded[col].mean()\n",
        "        std = train_encoded[col].std()\n",
        "        if std == 0: std = 1\n",
        "        train_encoded[col] = (train_encoded[col] - mean) / std\n",
        "        test_encoded[col] = (test_encoded[col] - mean) / std\n",
        "\n",
        "    # This step is used for the next l2_norm\n",
        "    #     For **categorical attributes**, use:\n",
        "    # - Distance = 1 if values are different\n",
        "    # - Distance = 0 if values are the same\n",
        "    \n",
        "    dummy_cols = [c for c in train_encoded.columns \n",
        "                  if c not in numerical_cols and c != label_col]\n",
        "    \n",
        "    scale_factor = 1.0 / np.sqrt(2)  # 0.7071\n",
        "    \n",
        "    train_encoded[dummy_cols] *= scale_factor\n",
        "    test_encoded[dummy_cols] *= scale_factor\n",
        "\n",
        "    y_train = train_encoded[label_col].values\n",
        "    X_train = train_encoded.drop(columns=[label_col]).values\n",
        "    \n",
        "    y_test = test_encoded[label_col].values\n",
        "    X_test = test_encoded.drop(columns=[label_col]).values\n",
        "\n",
        "\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# def z_normalize(X_train, X_test, feature_indices):\n",
        "#     \"\"\"\n",
        "#     Apply z-score normalization to specified features.\n",
        "    \n",
        "#     Parameters:\n",
        "#     -----------\n",
        "#     X_train, X_test : numpy arrays\n",
        "#     feature_indices : list of indices for numerical features\n",
        "    \n",
        "#     Returns:\n",
        "#     --------\n",
        "#     X_train_normalized, X_test_normalized : numpy arrays\n",
        "#     \"\"\"\n",
        "#     # TODO: Implement z-normalization\n",
        "#     pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 2: Implement k-NN Classifier\n",
        "Implement k-NN with L2 (Euclidean) distance:\n",
        "$$\\mathcal{D}_{L2}(\\mathbf{a}, \\mathbf{b}) = \\sqrt{\\sum_i (a_i - b_i)^2}$$\n",
        "\n",
        "For **categorical attributes**, use:\n",
        "- Distance = 1 if values are different\n",
        "- Distance = 0 if values are the same\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def l2_distance(a, b):\n",
        "    \"\"\"\n",
        "    Compute L2 (Euclidean) distance between two vectors.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    a, b : numpy arrays of same shape\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    distance : float\n",
        "    \"\"\"\n",
        "    # TODO: Implement L2 distance\n",
        "\n",
        "    distances = np.linalg.norm(a-b)\n",
        "    \n",
        "    return distances\n",
        "\n",
        "\n",
        "def knn_predict(X_train, y_train, X_test, k):\n",
        "    \"\"\"\n",
        "    Predict labels for test data using k-NN.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    X_train : numpy array of shape (n_train, n_features)\n",
        "    y_train : numpy array of shape (n_train,)\n",
        "    X_test : numpy array of shape (n_test, n_features)\n",
        "    k : int, number of neighbors\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    predictions : numpy array of shape (n_test,)\n",
        "    \"\"\"\n",
        "    # TODO: Implement k-NN prediction\n",
        "    # For each test sample:\n",
        "    #   1. Compute distance to all training samples\n",
        "    #   2. Find k nearest neighbors\n",
        "    #   3. Predict using majority voting\n",
        "    # pass\n",
        "    \n",
        "\n",
        "    # It is more convenient to use calculate the distance in matrix and vector\n",
        "    \n",
        "    num_test = X_test.shape[0]\n",
        "    num_train = X_train.shape[0]\n",
        "    y_pred = np.zeros(num_test)\n",
        "    dists = np.zeros((num_test, num_train))\n",
        "    for i in range(num_test):\n",
        "        # Broadcasting\n",
        "        # (N, D) - (D,) -> (N, D) \n",
        "        dists[i, :] = np.linalg.norm(X_train - X_test[i], axis=1)     \n",
        "        closest_y_indices = np.argsort(dists[i, :])[:k]\n",
        "        closest_y = y_train[closest_y_indices]\n",
        "\n",
        "        counts = np.bincount(closest_y.astype(int))\n",
        "    \n",
        "        y_pred[i] = np.argmax(counts)\n",
        "\n",
        "    return y_pred\n",
        "\n",
        "\n",
        "def compute_accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Compute classification accuracy.\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    accuracy : float (between 0 and 1)\n",
        "    \"\"\"\n",
        "    # TODO: Implement accuracy computation\n",
        "    accu =  np.mean(y_pred == y_true)\n",
        "\n",
        "    return accu\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 3: Evaluate on Lenses Dataset\n",
        "Test your k-NN implementation on the Lenses dataset for different values of k.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k=1: Accuracy = 1.0000\n",
            "k=3: Accuracy = 1.0000\n",
            "k=5: Accuracy = 0.5000\n",
            "k=7: Accuracy = 0.8333\n",
            "[3. 3. 3. 2. 3. 2.] [3. 1. 3. 2. 3. 2.]\n"
          ]
        }
      ],
      "source": [
        "# TODO: Evaluate k-NN on Lenses dataset\n",
        "# Try different values of k (e.g., 1, 3, 5, 7)\n",
        "\n",
        "k_values = [1, 3, 5, 7]\n",
        "lenses_results = []\n",
        "\n",
        "for k in k_values:\n",
        "    predictions = knn_predict(X_train_lenses, y_train_lenses, X_test_lenses, k)\n",
        "    accuracy = compute_accuracy(y_test_lenses, predictions)\n",
        "    lenses_results.append((k, accuracy))\n",
        "    print(f\"k={k}: Accuracy = {accuracy:.4f}\")\n",
        "# print(X_train_lenses, X_test_lenses)\n",
        "print(predictions,y_test_lenses)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Task 4: Evaluate on Credit Approval Dataset\n",
        "First preprocess the data, then evaluate k-NN.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Preprocess Credit Approval data\n",
        "X_train_credit, y_train_credit, X_test_credit, y_test_credit = preprocess_credit_data(\n",
        "    DATA_PATH + \"crx.data.training\",\n",
        "    DATA_PATH + \"crx.data.testing\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "k=1: Accuracy = 0.9529\n",
            "k=3: Accuracy = 0.8768\n",
            "k=5: Accuracy = 0.8605\n",
            "k=7: Accuracy = 0.8551\n"
          ]
        }
      ],
      "source": [
        "# TODO: Evaluate k-NN on Credit Approval dataset\n",
        "k_values = [1, 3, 5, 7]\n",
        "credit_results = []\n",
        "\n",
        "for k in k_values:\n",
        "    predictions = knn_predict(X_train_credit, y_train_credit, X_test_credit, k)\n",
        "    accuracy = compute_accuracy(y_test_credit, predictions)\n",
        "    credit_results.append((k, accuracy))\n",
        "    print(f\"k={k}: Accuracy = {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary and Discussion\n",
        "\n",
        "### Results Table\n",
        "\n",
        "| Dataset | k=1 | k=3 | k=5 | k=7 |\n",
        "|---------|-----|-----|-----|-----|\n",
        "| Lenses | 1 | 1 | 0.5 | 0.8333 |\n",
        "| Credit Approval | 0.9529 | 0.8768 | 0.8587 | 0.8551 |\n",
        "\n",
        "### Discussion\n",
        "*Answer these questions:*\n",
        "1. Which value of k works best for each dataset? Why do you think that is?\n",
        "2. How did preprocessing affect your results on the Credit Approval dataset?\n",
        "3. What are the trade-offs of using different values of k?\n",
        "4. What did you learn from this exercise?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. **Which value of k works best for each dataset? Why do you think that is?**\n",
        "\n",
        "Credit Approval Dataset: $k=1$ performed best with an accuracy of 0.9529. The accuracy consistently decreased as $k$ increased ($k=7$ dropped to 0.8551).\n",
        "\n",
        "Lenses Dataset: $k=1$ and $k=3$ performed best, both achieving 1.0000 accuracy. However, performance dropped drastically at $k=5$ (0.5000).\n",
        "\n",
        "\n",
        "2. **How did preprocessing affect your results on the Credit Approval dataset?**\n",
        "\n",
        "(1) Normalization (Z-Score): Since KNN relies on Euclidean distance, features with large magnitudes (like A15 or A14) would have completely dominated the distance calculation without normalization.\n",
        "\n",
        "(2) One-Hot Encoding: Converting categorical variables (like A6, A7) into binary vectors allowed us to mathematically calculate distances for non-numeric data.\n",
        "\n",
        "3. **What are the trade-offs of using different values of k?**\n",
        "\n",
        "\n",
        "Small $k$ (e.g., $k=1$): Pros: Low bias; captures intricate, local structures in the data.\n",
        "\n",
        "Cons: High variance; very sensitive to noise and outliers. If a test point is near a mislabeled training point, it will be misclassified.\n",
        "\n",
        "Large $k$ (e.g., $k=7$):Pros: Low variance; smoothes out noise and decision boundaries, making the model more robust to outliers.\n",
        "\n",
        "Cons: High bias; computational cost is slightly higher (conceptually), and it risks underfitting. As seen in the Lenses dataset ($k=5$), if $k$ is too large, it can \"drown out\" the signal from the true minority class by including too many neighbors from the majority class.\n",
        "\n",
        "\n",
        "4. **What did you learn from this exercise?**\n",
        "\n",
        "Metric Sensitivity: I learned that KNN is not a \"plug-and-play\" algorithm; it is highly sensitive to how distance is defined. Mixed-type data (continuous and categorical) requires careful weighting (like the $\\sqrt{2}$ scaling we discussed) to ensure consistency(Distance = 1 if they are not accurate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
